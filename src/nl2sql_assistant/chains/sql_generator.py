from __future__ import annotations

from dataclasses import dataclass

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import Runnable

from nl2sql_assistant.llm.ollama_client import get_chat_model
from nl2sql_assistant.prompts.sql_prompt import SQL_PROMPT


@dataclass(frozen=True)
class SQLGenResult:
    sql: str
    notes: str


def build_sql_chain() -> Runnable:
    """
    Chain:
      inputs -> PromptTemplate -> ChatOllama -> parse string

    We keep it minimal now. Later stages can switch to JSON output parsing.
    """
    model = get_chat_model()
    return SQL_PROMPT | model | StrOutputParser()


def postprocess_sql(raw: str) -> str:
    """
    Normalizes model output:
    - strips whitespace
    - removes trailing semicolon duplication (we'll add semicolon when needed)
    """
    sql = raw.strip()
    # Sometimes models add extra commentary; a strict prompt helps but we also harden here.
    # If there are multiple lines, keep them (SQL can be multi-line).
    return sql


def generate_sql(schema_text: str, question: str) -> SQLGenResult:
    chain = build_sql_chain()
    raw = chain.invoke({"schema_text": schema_text, "question": question})
    sql = postprocess_sql(raw)

    # Notes are intentionally simple in Stage 2.
    notes = "Generated by open-source LLM via Ollama. Review before running."
    return SQLGenResult(sql=sql, notes=notes)
